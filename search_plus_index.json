{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction "},"dockerrong-qi-nei-ying-yong-ri-zhi-shou-ji-fang-an.html":{"url":"dockerrong-qi-nei-ying-yong-ri-zhi-shou-ji-fang-an.html","title":"Docker容器内应用日志收集方案","keywords":"","body":" "},"ji-zhu-xuan-xing.html":{"url":"ji-zhu-xuan-xing.html","title":"技术选型","keywords":"","body":"在基于容器的微服务架构中，由于服务运行的位置、数量和时间的不确定性，传统用于虚拟机的性能监控和日志收集方式很难适应容器应用动态变化的特点。 应用日志的去向 容器挂载宿主机目录作为应用日志输出目录，日志落盘在宿主机。 应用日志标准输出到控制台，由 Docker log-driver 接管日志。 应用主动把日志输出到日志收集器，如 kafka、logstash、syslog 等。 应用日志输出至文件 集群中所有宿主机上需要运行采集工具，如 filebeat、logstash 等。 相较于传统虚拟机应用差别不大，但由于 Docker 的特点以及应用混搭在同一集群中，有以下弊端： 应用混搭同一集群，采集端需要采集所有日志，不同业务日志格式不尽相同，日志分割时规则l'j太多，太乱； 不能很好的记录容器的信息，如主机名； 不能很好的区日志属于哪个业务； 集群中所有宿主机都要部署日志采集端，管理不方便，小规模下能使用； 应用日志标准输出到控制台 Docker 从 1.12 开始支持Logging Driver，允许将Docker日志路由到指定的第三方日志转发层，可将日志转发到 AWS CloudWatch，Fluentd，syslog，GELF 或 NAT 服务器。 使用 logging drivers 比较简单好用，它们需要为每个容器指定，并且将需要在日志的接收端进行其他配置。 (1) 指定 Logging Driver 为 GELF 并传送到 logstash : docker run \\ --log-driver gelf \\ --log-opt gelf-address=udp://10.78.170.55:12201 \\ alpine echo hello world logstash 配置: input { gelf { host => \"0.0.0.0\" port => 12201 } } output { elasticsearch { hosts => [\"elasticsearch\"] workers=> 10 } } (2) 指定 Logging Driver 为 syslog 并传送到 logstash : docker run \\ --log-driver=syslog \\ --log-opt syslog-address=tcp://10.78.170.55:5000 \\ --log-opt syslog-facility=daemon \\ alpine echo hello world logstash 配置: input { syslog { host => \"0.0.0.0\" port => 5000 } } output { elasticsearch { hosts => [\"elasticsearch\"] workers=> 10 } } 如此这样运行每一个容器，结果是将 Docker 容器日志流输出到 logstash input syslog server，这些日志经过 logstash 处理后发送到 Elasticsearch 或 kafka。 此方案对应用要求低，只需要日志前台输出就可以收集，并不同业务应用日志不会混在一起，方便后期的处理。 关于日志完整性以及数据持久化问采集端 logstash 可以不做任何处转发至 kaf其他组件通过订阅 Kafka 实现更丰富的日志处理。例如：logstash分割聚合后入Elasticsearch，storm 计算处理进行告警，flume 采集处理至 hd进行日志落盘。 kubernetes 目前暂未支持 pod 级别的 log-driver，但社区有讨论方案，后续有望支持。 应用主动把日志输出到日志收集器 业务日志由进程直接写入 Kafka，其他组件通过订阅 Kafka 实现更丰富的日志处理。例如，使用 Flume 订阅 Kafka 并将日志写入 HDFS。logstash 订阅 Kakfa 分割聚合写入 ElasticSearch。 此方案是最完美的，但需要对业务应日志模块进行更改。 参考 基于 Docker 容器应用的日志收集方案：http://lonf.me/2017/07/13/Docker-Log-Collection/#容器挂载宿主机目录作为应用日志输出目录 "},"ji-zhu-xuan-xing/json-file-logging-driver.html":{"url":"ji-zhu-xuan-xing/json-file-logging-driver.html","title":"JSON File logging driver","keywords":"","body":"By default, Docker captures the standard output (and standard error) of all your containers, and writes them in files using the JSON format（Docker 默认为所有容器记录容器中的标准输出、标准错误输出，以JSON格式）. The JSON format annotates each line with its origin (stdoutorstderr) and its timestamp. Each log file contains information about only one container（每个log文件只包括一个容器的log信息）. Usage To use thejson-filedriver as the default logging driver, set thelog-driverandlog-optkeys to appropriate values in thedaemon.jsonfile, which is located in/etc/docker/on Linux hosts orC:\\ProgramData\\docker\\config\\daemon.jsonon Windows Server. For more about +configuring Docker usingdaemon.json, see +daemon.json. The following example sets the log driver tojson-fileand sets themax-sizeoption. { \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"10m\" } } Restart Docker for the changes to take effect for newly created containers. Existing containers will not use the new logging configuration. 上面的配置生效后对宿主机上所有容器生效，下面的方法只对该容器生效： You can set the logging driver for a specific container by using the--log-driverflag todocker createordocker run: $ docker run \\ -–log-driver json-file --log-opt max-size=10m \\ alpine echo hello world Options Thejson-filelogging driver supports the following logging options: Option Description Example value max-size The maximum size of the log before it is rolled. A positive integer plus a modifier representing the unit of measure (k,m, org). Defaults to -1 (unlimited). --log-opt max-size=10m max-file The maximum number of log files that can be present. If rolling the logs creates excess files, the oldest file is removed.Only effective whenmax-sizeis also set.A positive integer. Defaults to 1. --log-opt max-file=3 labels Applies when starting the Docker daemon. A comma-separated list of logging-related labels this daemon will accept. Used for advancedlog tag options. --log-opt labels=production_status,geo env Applies when starting the Docker daemon. A comma-separated list of logging-related environment variables this daemon will accept. Used for advancedlog tag options. --log-opt env=os,customer env-regex Similar to and compatible withenv. A regular expression to match logging-related environment variables. Used for advancedlog tag options. --log-opt env-regex=^(os customer). Examples This example starts analpinecontainer which can have a maximum of 3 log files no larger than 10 megabytes each. $ docker run -it --log-opt max-size=10m --log-opt max-file=3 alpine ash "},"ji-zhu-xuan-xing/graylog-extended-formatgelflogging-driver.html":{"url":"ji-zhu-xuan-xing/graylog-extended-formatgelflogging-driver.html","title":"Graylog Extended Format(GELF)logging driver","keywords":"","body":"Thegelflogging driver is a convenient format that is understood by a number of tools such asGraylog,Logstash, andFluentd. Many tools use this format. In GELF, every log message is a dict with the following fields: version host (who sent the message in the first place) timestamp short and long version of the message any custom fields you configure yourself Usage To use thegelfdriver as the default logging driver, set thelog-driverandlog-optkeys to appropriate values in thedaemon.jsonfile, which is located in/etc/docker/on Linux hosts orC:\\ProgramData\\docker\\config\\daemon.jsonon Windows Server. For more about configuring Docker usingdaemon.json, seedaemon.json. The following example sets the log driver togelfand sets thegelf-addressoption. { \"log-driver\": \"gelf\", \"log-opts\": { \"gelf-address\": \"udp://1.2.3.4:12201\" } } Restart Docker for the changes to take effect. To usegelfas the default logging driver for new containers, pass the--log-driverand--log-optoptions to the Docker daemon: dockerd -–log-driver gelf –-log-opt gelf-address=udp://1.2.3.4:12201 \\ To make the configuration permanent, you can configure it in/etc/docker/daemon.json: { \"log-driver\": \"gelf\", \"log-opts\": { \"gelf-address\": \"udp://1.2.3.4:12201\" } } You can set the logging driver for a specific container by setting the--log-driverflag when usingdocker createordocker run: $ docker run \\ -–log-driver gelf –-log-opt gelf-address=udp://1.2.3.4:12201 \\ alpine echo hello world GELF options Thegelflogging driver supports the following options: Option Required Description Example value gelf-address required The address of the GELF server.tcpandudpare the only supported URI specifier and you must specify the port. --log-opt gelf-address=udp://192.168.0.42:12201 gelf-compression-type optional UDP OnlyThe type of compression the GELF driver uses to compress each log message. Allowed values aregzip,zlibandnone. The default isgzip. --log-opt gelf-compression-type=gzip gelf-compression-level optional UDP OnlyThe level of compression whengziporzlibis thegelf-compression-type. An integer in the range of-1to9(BestCompression). Default value is 1 (BestSpeed). Higher levels provide more compression at lower speed. Either-1or0disables compression. --log-opt gelf-compression-level=2 gelf-tcp-max-reconnect optional TCP OnlyThe maximum number of reconnection attempts when the connection drop. An positive integer. Default value is 3. --log-opt gelf-tcp-max-reconnect=3 gelf-tcp-reconnect-delay optional TCP OnlyThe number of seconds to wait between reconnection attempts. A positive integer. Default value is 1. --log-opt gelf-tcp-reconnect-delay=1 tag optional A string that is appended to theAPP-NAMEin thegelfmessage. By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to thelog tag option documentationfor customizing the log tag format. --log-opt tag=mailer labels optional Applies when starting the Docker daemon. A comma-separated list of logging-related labels this daemon will accept. Adds additional key on theextrafields, prefixed by an underscore (_). Used for advancedlog tag options. --log-opt labels=production_status,geo env optional Applies when starting the Docker daemon. A comma-separated list of logging-related environment variables this daemon will accept. Adds additional key on theextrafields, prefixed by an underscore (_). Used for advancedlog tag options. --log-opt env=os,customer env-regex optional Similar to and compatible withenv. A regular expression to match logging-related environment variables. Used for advancedlog tag options. --log-opt env-regex=^(os customer). Examples This example configures the container to use the GELF server running at192.168.0.42on port12201. $ docker run -dit \\ --log-driver=gelf \\ --log-opt gelf-address=udp://192.168.0.42:12201 \\ alpine sh "},"ji-zhu-xuan-xing/syslog-logging-driver.html":{"url":"ji-zhu-xuan-xing/syslog-logging-driver.html","title":"Syslog logging driver","keywords":"","body":"Thesysloglogging driver routes logs to asyslogserver. Thesyslogprotocol uses a raw string as the log message and supports a limited set of metadata. The syslog message must be formatted in a specific way to be valid. From a valid message, the receiver can extract the following information: priority: the logging level, such asdebug,warning,error,info. timestamp: when the event occurred. hostname: where the event happened. facility: which subsystem logged the message, such asmailorkernel. process name and process ID (PID): The name and ID of the process that generated the log. The format is defined inRFC 5424and Docker’s syslog driver implements theABNF referencein the following way: TIMESTAMP SP HOSTNAME SP APP-NAME SP PROCID SP MSGID + + + | + | | | | | | | | | | +------------+ +----+ | +----+ +---------+ v v v v v 2017-04-01T17:41:05.616647+08:00 a.vm {taskid:aa,version:} 1787791 {taskid:aa,version:} Usage To use thesyslogdriver as the default logging driver, set thelog-driverandlog-optkeys to appropriate values in thedaemon.jsonfile, which is located in/etc/docker/on Linux hosts orC:\\ProgramData\\docker\\config\\daemon.jsonon Windows Server. For more about configuring Docker usingdaemon.json, seedaemon.json. The following example sets the log driver tosyslogand sets thesyslog-addressoption. { \"log-driver\": \"syslog\", \"log-opts\": { \"syslog-address\": \"udp://1.2.3.4:1111\" } } Restart Docker for the changes to take effect. Note: The syslog-address supports both UDP and TCP. You can set the logging driver for a specific container by using the--log-driverflag todocker createordocker run: docker run \\ -–log-driver syslog –-log-opt syslog-address=udp://1.2.3.4:1111 \\ alpine echo hello world Options The following logging options are supported as options for thesysloglogging driver. They can be set as defaults in thedaemon.json, by adding them as key-value pairs to thelog-optsJSON array. They can also be set on a given container by adding a--log-opt =flag for each option when starting the container. Option Description Example value syslog-address The address of an externalsyslogserver. The URI specifier may be[tcp、udp、tcp+tls]://host:port,unix://path, orunixgram://path. If the transport istcp,udp, ortcp+tls, the default port is514. --log-opt syslog-address=tcp+tls://192.168.1.3:514,--log-opt syslog-address=unix:///tmp/syslog.sock syslog-facility Thesyslogfacility to use. Can be the number or name for any validsyslogfacility. See thesyslog documentation. --log-opt syslog-facility=daemon syslog-tls-ca-cert The absolute path to the trust certificates signed by the CA.Ignored if the address protocol is nottcp+tls. --log-opt syslog-tls-ca-cert=/etc/ca-certificates/custom/ca.pem syslog-tls-cert The absolute path to the TLS certificate file.Ignored if the address protocol is nottcp+tls. --log-opt syslog-tls-cert=/etc/ca-certificates/custom/cert.pem syslog-tls-key The absolute path to the TLS key file.Ignored if the address protocol is nottcp+tls. --log-opt syslog-tls-key=/etc/ca-certificates/custom/key.pem syslog-tls-skip-verify If set totrue, TLS verification is skipped when connecting to thesyslogdaemon. Defaults tofalse.Ignored if the address protocol is nottcp+tls. --log-opt syslog-tls-skip-verify=true tag A string that is appended to theAPP-NAMEin thesyslogmessage. By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to thelog tag option documentationfor customizing the log tag format. --log-opt tag=mailer syslog-format Thesyslogmessage format to use. If not specified the local UNIX syslog format is used, without a specified hostname. Specifyrfc3164for the RFC-3164 compatible format,rfc5424for RFC-5424 compatible format, orrfc5424microfor RFC-5424 compatible format with microsecond timestamp resolution. --log-opt syslog-format=rfc5424micro labels Applies when starting the Docker daemon. A comma-separated list of logging-related labels this daemon will accept. Used for advancedlog tag options. --log-opt labels=production_status,geo env Applies when starting the Docker daemon. A comma-separated list of logging-related environment variables this daemon will accept. Used for advancedlog tag options. --log-opt env=os,customer env-regex Applies when starting the Docker daemon. Similar to and compatible withenv. A regular expression to match logging-related environment variables. Used for advancedlog tag options. `--log-opt env-regex=^(os\\ customer)` "},"ji-zhu-xuan-xing/fluentd-logging-driver.html":{"url":"ji-zhu-xuan-xing/fluentd-logging-driver.html","title":"Fluentd logging driver","keywords":"","body":"Thefluentdlogging driver sends container logs to the Fluentd collector as structured log data. Then, users can use any of the various output plugins of Fluentd to write these logs to various destinations. In addition to the log message itself, thefluentdlog driver sends the following metadata in the structured log message: Field Description container_id The full 64-character container ID. container_name The container name at the time it was started. If you usedocker renameto rename a container, the new name is not reflected in the journal entries. source stdoutorstderr Thedocker logscommand is not available for this logging driver. Usage Some options are supported by specifying--log-optas many times as needed: fluentd-address: specify a socket address to connect to the Fluentd daemon, exfluentdhost:24224orunix:///path/to/fluentd.sock tag: specify a tag for fluentd message, which interprets some markup To use thefluentddriver as the default logging driver, set thelog-driverandlog-optkeys to appropriate values in thedaemon.jsonfile, which is located in/etc/docker/on Linux hosts orC:\\ProgramData\\docker\\config\\daemon.jsonon Windows Server. For more about +configuring Docker usingdaemon.json, see +daemon.json. The following example sets the log driver tofluentdand sets thefluentd-addressoption. { \"log-driver\": \"fluentd\", \"log-opts\": { \"fluentd-address\": \"fluentdhost:24224\" } } Restart Docker for the changes to take effect. To set the logging driver for a specific container, pass the--log-driveroption todocker run: docker run --log-driver=fluentd ... Before using this logging driver, launch a Fluentd daemon. The logging driver connects to this daemon throughlocalhost:24224by default. Use thefluentd-addressoption to connect to a different address. docker run --log-driver=fluentd --log-opt fluentd-address=fluentdhost:24224 If container cannot connect to the Fluentd daemon, the container stops immediately unless thefluentd-async-connectoption is used. Options Users can use the--log-opt NAME=VALUEflag to specify additional Fluentd logging driver options. fluentd-address By default, the logging driver connects tolocalhost:24224. Supply thefluentd-addressoption to connect to a different address.tcp(default) andunixsockets are supported. docker run --log-driver=fluentd --log-opt fluentd-address=fluentdhost:24224 docker run --log-driver=fluentd --log-opt fluentd-address=tcp://fluentdhost:24224 docker run --log-driver=fluentd --log-opt fluentd-address=unix:///path/to/fluentd.sock Two of the above specify the same address, becausetcpis default. tag By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to thelog tag option documentationfor customizing the log tag format. labels, env, and env-regex Thelabelsandenvoptions each take a comma-separated list of keys. If there is collision betweenlabelandenvkeys, the value of theenvtakes precedence. Both options add additional fields to the extra attributes of a logging message. Theenv-regexoption is similar to and compatible withenv. Its value is a regular expression to match logging-related environment variables. It is used for advancedlog tag options. fluentd-async-connect Docker connects to Fluentd in the background. Messages are buffered until the connection is established. fluentd-buffer-limit The amount of data to buffer before flushing to disk. Defaults to the amount of RAM available to the container. fluentd-retry-wait How long to wait between retries. Defaults to 1 second. fluentd-max-retries The maximum number of retries. Defaults to 10. Fluentd daemon management with Docker AboutFluentditself, seethe project webpageandits documents. To use this logging driver, start thefluentddaemon on a host. We recommend that you usethe Fluentd docker image. This image is especially useful if you want to aggregate multiple container logs on each host then, later, transfer the logs to another Fluentd node to create an aggregate store. Test container loggers Write a configuration file (test.conf) to dump input logs: @type forward @type stdout Launch Fluentd container with this configuration file: $ docker run -it -p 24224:24224 -v /path/to/conf/test.conf:/fluentd/etc/test.conf -e FLUENTD_CONF=test.conf fluent/fluentd:latest Start one or more containers with thefluentdlogging driver: $ docker run --log-driver=fluentd your/application "},"ji-zhu-xuan-xing/journald-logging-driver.html":{"url":"ji-zhu-xuan-xing/journald-logging-driver.html","title":"Journald logging driver","keywords":"","body":"Thejournaldlogging driver sends container logs to thesystemdjournal. Log entries can be retrieved using thejournalctlcommand, through use of thejournalAPI, or using thedocker logscommand. In addition to the text of the log message itself, thejournaldlog driver stores the following metadata in the journal with each message: Field Description CONTAINER_ID The container ID truncated to 12 characters. CONTAINER_ID_FULL The full 64-character container ID. CONTAINER_NAME The container name at the time it was started. If you usedocker renameto rename a container, the new name is not reflected in the journal entries. CONTAINER_TAG The container tag (log tag option documentation). CONTAINER_PARTIAL_MESSAGE A field that flags log integrity. Improve logging of long log lines. Usage To use thejournalddriver as the default logging driver, set thelog-driverandlog-optkeys to appropriate values in thedaemon.jsonfile, which is located in/etc/docker/on Linux hosts orC:\\ProgramData\\docker\\config\\daemon.jsonon Windows Server. For more about +configuring Docker usingdaemon.json, see + daemon.json. The following example sets the log driver tojournald: { \"log-driver\" : \"journald\" } Restart Docker for the changes to take effect. To configure the logging driver for a specific container, use the--log-driverflag on thedocker runcommand. $ docker run --log-driver = journald ... Options Use the--log-opt NAME=VALUEflag to specify additionaljournaldlogging driver options. tag Specify template to setCONTAINER_TAGvalue injournaldlogs. Refer to log tag option documentation to customize the log tag format. labels,env, andeng-regex Thelabelsandenvoptions each take a comma-separated list of keys. If there is collision betweenlabelandenvkeys, the value of theenvtakes precedence. Each option adds additional metadata to the journal with each message. env-regexis similar to and compatible withenv. Set it to a regular expression to match logging-related environment variables. It is used for advanced log tag options. Note regarding container names The value logged in theCONTAINER_NAMEfield is the name of the container that was set at startup. If you usedocker renameto rename a container, the new nameis not reflectedin the journal entries. Journal entries will continue to use the original name. Retrieve log messages withjournalctl Use thejournalctlcommand to retrieve log messages. You can apply filter expressions to limit the retrieved messages to those associated with a specific container: $ sudo journalctl CONTAINER_NAME=webserver You can use additional filters to further limit the messages retrieved. The-bflag only retrieves messages generated since the last system boot: $ sudo journalctl -b CONTAINER_NAME=webserver The-oflag specifies the format for the retried log messages. Use-o jsonto return the log messages in JSON format. $ sudo journalctl -o json CONTAINER_NAME=webserver Retrieve log messages with thejournalAPI This example uses thesystemdPython module to retrieve container logs: import systemd.journal reader = systemd.journal.Reader() reader.add_match('CONTAINER_NAME=web') for msg in reader: print '{CONTAINER_ID_FULL}: {MESSAGE}'.format(**msg) "},"ji-zhu-xuan-xing/other-online-logging-driver.html":{"url":"ji-zhu-xuan-xing/other-online-logging-driver.html","title":"Other online logging driver","keywords":"","body":"另外存在公有云中的日志服务器，比如AWS CloudWatch、Google Cloud: Logentries logging driver: https://docs.docker.com/engine/admin/logging/logentries/ Amazon CloudWatch Logs logging driver: https://docs.docker.com/engine/admin/logging/awslogs/ Google Cloud Logging driver: https://docs.docker.com/engine/admin/logging/gcplogs/ "},"ji-yu-xxx-de-ri-zhi-shou-ji.html":{"url":"ji-yu-xxx-de-ri-zhi-shou-ji.html","title":"基于fluentd的日志收集","keywords":"","body":"Fluentd Architecture Overview Fluentd is an open source log collector, which lets you unify the data collection and consumption for a better use and understanding of data. Fluentd treats logs as JSON, a popular machine-readable format. It is written primarily in C with a thin-Ruby wrapper that gives users flexibility. Fluentd’s scalability has been proven in the field: its largest user currently collects logs from 500,000+ servers. Fluentd是一个免费，而且完全开源的日志管理工具，简化了日志的收集、处理、和存储，你可以不需要再维护编写特殊的日志处理脚本。Fluentd的性能已经在各领域得到了证明：目前最大的用户从5000+服务器收集日志，每天5TB的数据量，在高峰时间处理50,000条信息每秒。 Before Fluentd： After Fluentd： Unified Logging with JSON Fluentd tries to structure data as JSON as much as possible: this allows Fluentd to unify all facets of processing log data: collecting, filtering, buffering, and outputting logs across multiple sources and destinations (Unified Logging Layer). The downstream data processing is much easier with JSON, since it has enough structure to be accessible while retaining flexible schemas. Pluggable Architecture Fluentd has a flexible plugin system that allows the community to extend its functionality. Our 500+ community-contributed plugins connect dozens of data sources and data outputs. By leveraging the plugins, you can start making better use of your logs right away. Minimum Resources Required Fluentd is written in a combination of C language and Ruby, and requires very little system resource. The vanilla（简朴、平常） instance runs on 30-40MB of memory and can process 13,000 events/second/core. If you have more tighter memory requirement (-450kb), check out Fluent Bit, the lightweight forwarder for Fluentd. Built-in Reliability Fluentd supports memory and file-based buffering to prevent inter-node data loss. Fluentd also support robust failover and can be set up for high availability. 2,000+ data-driven companies rely on Fluentd to differentiate their products and services through a better use and understanding of their log data. Life of a Fluentd event The following content describe a global overview of how events are processed by Fluentd using examples. It covers the complete cycle including Setup, Inputs, Filters, Matches and Labels. Basic Setup The configuration files is the fundamental piece to connect all things together, as it allows to define which Inputs or listeners Fluentd will have and set up common matching rules to route the Event data to a specific Output. We will use the in_http and the out_stdout plugins as examples to describe the events cycle. The following is a basic definition on the configuration file to specify an http input, for short: we will be listening for HTTP Requests: @type http port 8888 bind 0.0.0.0 This definition specifies that a HTTP server will be listening on TCP port 8888. Now let’s define a Matching rule and a desired output that will just print the data that arrived on each incoming request to standard output: @type stdout The Match directive sets a rule that matches each Incoming event that arrives with a Tag equal to test.cycle will use the Output plugin type called stdout. At this point we have an Input type, a Match and an Output. Let’s test the setup using curl: $ curl -i -X POST -d 'json={\"action\":\"login\",\"user\":2}' http://localhost:9880/test.cycle HTTP/1.1 200 OK Content-type: text/plain Connection: Keep-Alive Content-length: 0 On the Fluentd server side the output should look like this: $ bin/fluentd -c in_http.conf 2015-01-19 12:37:41 -0600 [info]: reading config file path=\"in_http.conf\" 2015-01-19 12:37:41 -0600 [info]: starting fluentd-0.12.3 2015-01-19 12:37:41 -0600 [info]: using configuration file: @type http bind 0.0.0.0 port 8888 @type stdout 2015-01-19 12:37:41 -0600 [info]: adding match pattern=\"test.cycle\" type=\"stdout\" 2015-01-19 12:37:41 -0600 [info]: adding source type=\"http\" 2015-01-19 12:39:57 -0600 test.cycle: {\"action\":\"login\",\"user\":2} Event structure A Fluentd event consists of a tag, time and record. tag: Where an event comes from. For message routing time: When an event happens. Epoch time record: Actual log content. JSON object The input plugin is responsible for generating Fluentd events from specified data sources.For example,in_tailgenerates events from text lines. If you have the following line in apache logs: 192.168.0.1 - - [28/Feb/2013:12:00:00 +0900] \"GET / HTTP/1.1\" 200 777 the following event is generated: tag: apache.access # set by configuration time: 1362020400 # 28/Feb/2013:12:00:00 +0900 record: {\"user\":\"-\",\"method\":\"GET\",\"code\":200,\"size\":777,\"host\":\"192.168.0.1\",\"path\":\"/\"} Processing Events When a Setup is defined, the Router Engine already contains several rules to apply for different input data. Internally an Event will pass through a chain of procedures that may alter the Events cycle. Now we will expand the previous basic example and add more steps in our Setup to demonstrate how the Events cycle can be altered. We will do this through the new Filters implementation. Filters A Filter aims to behave like a rule to either accept or reject an event. The following configuration adds a Filter definition: @type http port 8888 bind 0.0.0.0 @type grep exclude1 action logout @type stdout As you can see, the new Filter definition added will be a mandatory step before passing control to the Match section. The Filter basically accepts or rejects the Event based on it type and the defined rule. For our example we want to discard any user logoutaction, we only care about the loginaction. The way this is accomplished is by the Filter doing a grepthat will exclude any message where the actionkey has the string value “logout”. From a Terminal, run the following two curlcommands (please note that each one contains a different actionvalue): $ curl -i -X POST -d 'json={\"action\":\"login\",\"user\":2}' http://localhost:8880/test.cycle HTTP/1.1 200 OK Content-type: text/plain Connection: Keep-Alive Content-length: 0 $ curl -i -X POST -d 'json={\"action\":\"logout\",\"user\":2}' http://localhost:8880/test.cycle HTTP/1.1 200 OK Content-type: text/plain Connection: Keep-Alive Content-length: 0 Now looking at the Fluentd service output we can see that only the event with actionequal to “login” is matched. The logoutEvent was discarded: $ bin/fluentd -c in_http.conf 2015-01-19 12:37:41 -0600 [info]: reading config file path=\"in_http.conf\" 2015-01-19 12:37:41 -0600 [info]: starting fluentd-0.12.4 2015-01-19 12:37:41 -0600 [info]: using configuration file: @type http bind 0.0.0.0 port 9880 @type grep exclude1 action logout @type stdout 2015-01-19 12:37:41 -0600 [info]: adding filter pattern=\"test.cycle\" type=\"grep\" 2015-01-19 12:37:41 -0600 [info]: adding match pattern=\"test.cycle\" type=\"stdout\" 2015-01-19 12:37:41 -0600 [info]: adding source type=\"http\" 2015-01-27 01:27:11 -0600 test.cycle: {\"action\":\"login\",\"user\":2} As you can see, the Events follow a step-by-step cycle where they are processed in order from top to bottom. The new engine on Fluentd allows integrating as many Filters as needed. Considering that the configuration file might grow and start getting a bit complex, a new feature called Labels has been added that aims to help manage this complexity. Labels The Labels implementation aims to reduce configuration file complexity and allows to define new Routing sections that do not follow the top to bottom order, instead acting like linked references. Using the previous example we will modify the setup as follows: @type http bind 0.0.0.0 port 8880 @label @STAGING @type grep exclude1 action login @type grep exclude1 action logout @type stdout This new configuration contains a @label key on the sourceindicating that any further steps take place on the STAGING Label section. Every Event reported on the Source is routed by the Routing engine and continue processing on STAGING, skipping the old filter definition. Buffers In this example, we use stdoutnon-buffered output, but in production buffered outputs are often necessary, e.g. forward, mongodb, s3and etc. Buffered output plugins store received events into buffers and are then written out to a destination after meeting flush conditions. Using buffered output you don’t see recieved events immediately, unlike stdoutnon-buffered output. Buffers are important for reliability and throughput. See Output and Buffer articles. Execution unit This section describes the internal implementation. Fluentd’s events are processed on input plugin thread by default. For example, if you have in_tail -> filter_grep -> out_stdout pipeline, this pipeline is executed on in_tail’s thread. The important point is filter_grep and out_stdout plugins don’t have own thread for processing. For Buffered Output, output plugin has own threads for flushing buffer. For example, if you have in_tail -> filter_grep -> out_forward pipeline, this pipeline is executed on in_tail’s thread and out_forward’s flush thread. in_tail’s thread processes events until events are written into buffer. Buffer flush and its error handling is the output responsibility. This de-couples the input/output and this model has merits, separate responsibilities, easy error handling, good performance. Conclusion Once the events are reported by the Fluentd engine on the Source they can be processed step by step or inside a referenced Label, with any Event being filtered out at any moment. The new Routing engine behavior aims to provide more flexibility and simplifies the processing before events reach the Output plugin. 参考 Docker Logging via EFK (Elasticsearch + Fluentd + Kibana) Stack with Docker Compose：https://docs.fluentd.org/v0.12/articles/docker-logging-efk-compose Docker Logging Driver and Fluentd：https://docs.fluentd.org/v0.12/articles/docker-logging Docker Logging：https://www.fluentd.org/guides/recipes/docker-logging What is Fluentd?：https://www.fluentd.org/architecture Why Use Fluentd?：https://www.fluentd.org/why Unified Logging Layer: Turning Data into Action：https://www.fluentd.org/blog/unified-logging-layer Quickstart Guide：https://docs.fluentd.org/v0.12/articles/quickstart kubernetes Logging Architecture：https://kubernetes.io/docs/concepts/cluster-ad "},"ji-yu-xxx-de-ri-zhi-shou-ji/ji-yu-elasticsearch-+-fluentd-+-kibana-de-ri-zhi-fang-an-shi-jian.html":{"url":"ji-yu-xxx-de-ri-zhi-shou-ji/ji-yu-elasticsearch-+-fluentd-+-kibana-de-ri-zhi-fang-an-shi-jian.html","title":"基于ElasticSearch+Fluentd+Kibana的日志方案实践","keywords":"","body":"Elasticsearch is an open source search engine known for its ease of use. Kibana is an open source Web UI that makes Elasticsearch user friendly for marketers, engineers and data scientists alike. By combining these three tools EFK (Elasticsearch + Fluentd + Kibana) we get a scalable, flexible, easy to use log collection and analytics pipeline. In this article, we will set up 4 containers, each includes: Apache HTTP Server Fluentd Elasticsearch Kibana All of httpd’s logs will be ingested into Elasticsearch + Kibana, via Fluentd. Step 0: prepare docker-compose.yml First, please prepare docker-compose.yml for Docker Compose. Docker Compose is a tool for defining and running multi-container Docker applications. With the YAML file below, you can create and start all the services (in this case, Apache, Fluentd, Elasticsearch, Kibana) by one command. version: '2' services: web: image: httpd ports: - \"80:80\" links: - fluentd logging: driver: \"fluentd\" options: fluentd-address: localhost:24224 tag: httpd.access fluentd: build: ./fluentd volumes: - ./fluentd/conf:/fluentd/etc links: - \"elasticsearch\" ports: - \"24224:24224\" - \"24224:24224/udp\" elasticsearch: image: elasticsearch expose: - 9200 ports: - \"9200:9200\" kibana: image: kibana links: - \"elasticsearch\" ports: - \"5601:5601\" loggingsection (check Docker Compose documentation ) ofwebcontainer specifies Docker Fluentd Logging Driver as a default container logging driver. All of the logs fromwebcontainer will be automatically forwarded to host:port specified byfluentd-address. Step 1: Prepare Fluentd image with your Config + Plugin Then, please preparefluentd/Dockerfilewith the following content, to use Fluentd’s official Docker image and additionally install Elasticsearch plugin. # fluentd/Dockerfile FROM fluent/fluentd:v0.12-debian RUN [\"gem\", \"install\", \"fluent-plugin-elasticsearch\", \"--no-rdoc\", \"--no-ri\", \"--version\", \"1.9.2\"] Then, please prepare Fluentd’s configuration filefluentd/conf/fluent.conf. in_forward plugin is used for receive logs from Docker logging driver, and out_elasticsearch is for forwarding logs to Elasticsearch. # fluentd/conf/fluent.conf @type forward port 24224 bind 0.0.0.0 @type copy @type elasticsearch host elasticsearch port 9200 logstash_format true logstash_prefix fluentd logstash_dateformat %Y%m%d include_tag_key true type_name access_log tag_key @log_name flush_interval 1s @type stdout Step 2: Start Containers Let’s start all of the containers, with just one command. $ docker-compose up You can check to see if 4 containers are running bydocker pscommand. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2d28323d77a3 httpd \"httpd-foreground\" About an hour ago Up 43 seconds 0.0.0.0:80->80/tcp dockercomposeefk_web_1 a1b15a7210f6 dockercomposeefk_fluentd \"/bin/sh -c 'exec ...\" About an hour ago Up 45 seconds 5140/tcp, 0.0.0.0:24224->24224/tcp, 0.0.0.0:24224->24224/udp dockercomposeefk_fluentd_1 01e43b191cc1 kibana \"/docker-entrypoin...\" About an hour ago Up 45 seconds 0.0.0.0:5601->5601/tcp dockercomposeefk_kibana_1 b7b439415898 elasticsearch \"/docker-entrypoin...\" About an hour ago Up 50 seconds 0.0.0.0:9200->9200/tcp, 9300/tcp dockercomposeefk_elasticsearch_1 Step 3: Generate httpd Access Logs Let’s access tohttpdto generate some access logs.curlcommand is always your friend. $ repeat 10 curl http://localhost:80/ It works! It works! It works! It works! It works! It works! It works! It works! It works! It works! Step 4: Confirm Logs from Kibana Please go tohttp://localhost:5601/with your browser. Then, you need to set up the index name pattern for Kibana. Please specifyfluentd-*toIndex name or patternand pressCreatebutton. Then, go toDiscovertab to seek for the logs. As you can see, logs are properly collected into Elasticsearch + Kibana, via Fluentd. Conclusion This article explains how to collect logs from Apache to EFK (Elasticsearch + Fluentd + Kibana). The example code is available in this repository. https://github.com/kzk/docker-compose-efk "},"dockerrong-qi-de-jian-kong.html":{"url":"dockerrong-qi-de-jian-kong.html","title":"Docker容器的监控","keywords":"","body":" 参考 Docker容器的自动化监控实现：http://www.dockerinfo.net/4361.html 使用 Prometheus 监控 Docker 容器：http://dockone.io/article/171 分享Docker监控体系（Kubernetes Mesos监控）：http://www.dockerinfo.net/1718.html "},"dockerrong-qi-de-jian-kong/rong-qi-jian-kong-xuan-xing.html":{"url":"dockerrong-qi-de-jian-kong/rong-qi-jian-kong-xuan-xing.html","title":"容器监控选型","keywords":"","body":"The number of monitoring solutions is daunting. New solutions are coming on the scene continuously, and existing solutions evolve in functionality. Rather than looking at each solution in depth, I’ve taken the approach of drawing high-level comparisons. With this approach, readers can hopefully “narrow the list” and do more serious evaluations of solutions best suited to their own needs. The monitoring solutions covered here include: Native Docker cAdvisor Scout Pingdom Datadog Sysdig Prometheus Heapster / Grafana ELK stack Sensu In the following sections, I suggest a framework for comparing monitoring solutions, present a high-level comparison of each, and then discuss each solution in more detail by addressing how each solution works with Rancher. I also cover a few additional solutions you may have come across that did not make my top 10. A Framework for Comparison A challenge with objectively comparing monitoring solutions is that architectures, capabilities, deployment models, and costs can vary widely. One solution may extract and graph Docker-related metrics from a single host while another aggregates data from many hosts, measures application response times, and sends automated alerts under particular conditions. Having a framework is useful when comparing solutions. I’ve somewhat arbitrarily proposed the following tiers of functionality that most monitoring solutions have in common as a basis for my comparison. Like any self-respecting architectural stack, this one has seven layers. Host Agents – The host agent represents the “arms and legs” of the monitoring solution, extracting time-series data from various sources like APIs and log files. Agents are usually installed on each cluster host (either on-premises or cloud-resident) and are themselves often packaged as Docker containers for ease of deployment and management. Data gathering framework – While single-host metrics are sometimes useful, administrators likely need a consolidated view of all hosts and applications. Monitoring solutions typically have some mechanism to gather data from each host and persist it in a shared data store. Datastore – The datastore may be a traditional database, but more commonly it is some form of scalable, distributed database optimized for time-series data comprised of key-value pairs. Some solutions have native datastores while others leverage pluggable open-source datastores. Aggregation engine – The problem with storing raw metrics from dozens of hosts is that the amount of data can become overwhelming. Monitoring frameworks often provide data aggregation capabilities, periodically crunching raw data into consolidated metrics (like hourly or daily summaries), purging old data that is no longer needed, or re-factoring data in some fashion to support anticipated queries and analysis. Filtering & Analysis – A monitoring solution is only as good as the insights you can gain from the data. Filtering and analysis capabilities vary widely. Some solutions support a few pre-packaged queries presented as simple time-series graphs, while others have customizable dashboards, embedded query languages, and sophisticated analytic functions. Visualization tier – Monitoring tools usually have a visualization tier where users can interact with a web interface to generate charts, formulate queries and, in some cases, define alerting conditions. The visualization tier may be tightly coupled with the filtering and analysis functionality, or it may be separate depending on the solution. Alerting & Notification – Few administrators have time to sit and monitor graphs all day. Another common feature of monitoring systems is an alerting subsystem that can provide notification if pre-defined thresholds are met or exceeded. Beyond understanding how each monitoring solution implements the basic capabilities above, users will be interested in other aspects of the monitoring solution as well: Completeness of the solution Ease of installation and configuration Details about the web UI Ability to forward alerts to external services Level of community support and engagement (for open-source projects) Availability in Rancher Catalog Support for monitoring non-container environments and apps Native Kubernetes support (Pods, Services, Namespaces, etc.) Extensibility (APIs, other interfaces) Deployment model (self-hosted, cloud) Cost, if applicable Comparing Our 10 Monitoring Solutions The diagram below shows a high-level view of how our 10 monitoring solutions map to our seven-layer model, which components implement the capabilities at each layer, and where the components reside. Each framework is complicated, and this is a simplification to be sure, but it provides a useful view of which component does what. Read on for additional detail. Additional attributes of each monitoring solution are presented in a summary fashion below. For some solutions, there are multiple deployment options, so the comparisons become a little more nuanced. Looking at Each Solution in More Depth DOCKER STATS https://www.docker.com/docker-community At the most basic level, Docker provides built-in command monitoring for Docker hosts via thedocker statscommand. Administrators can query the Docker daemon and obtain detailed, real-time information about container resource consumption metrics, including CPU and memory usage, disk and network I/O, and the number of running processes. Docker stats leverages the Docker Engine API to retrieve this information. Docker stats has no notion of history, and it can only monitor a single host, but clever administrators can write scripts to gather metrics from multiple hosts. Docker stats is of limited use on its own, butdocker statsdata can be combined with other data sources like Docker log files anddocker eventsto feed higher level monitoring services. Docker only knows about metrics reported by a single host, so Docker stats is of limited use monitoring Kubernetes or Swarm clusters with multi-host application services. With no visualization interface, no aggregation, no datastore, and no ability to collect data from multiple hosts, Docker stats does not fare well against our seven-layer model. Because Rancher runs on Docker, basicdocker statsfunctionality is automatically available to Rancher users. 我将讨论的第一个工具是Docker本身。你可能不知道Docker客户端已经提供了基本的命令行工具来检查容器的资源消耗。想要查看容器统计信息只需运行docker stats [CONTAINER_NAME]。这样就可以查看每个容器的CPU利用率、内存的使用量以及可用内存总量。请注意，如果你没有限制容器内存，那么该命令将显示您的主机的内存总量。但它并不意味着你的每个容器都能访问那么多的内存。另外，还可以看容器通过网络发送和接收的数据总量。 CADVISOR https://github.com/google/cadvisor cAdvisor (container advisor) is an open-source project that like Docker stats provides users with resource usage information about running containers. cAdvisor was originally developed by Google to manage its lmctfy containers, but it now supports Docker as well. It is implemented as a daemon process that collects, aggregates, processes, and exports information about running containers. cAdvisor exposes a web interface and can generate multiple graphs but, like Docker stats, it monitors only a single Docker host. It can be installed on a Docker machine either as a container or natively on the Docker host itself. cAdvisor itself only retains information for 60 seconds. cAdvisor needs to be configured to log data to an external datastore. Datastores commonly used with cAdvisor data include Prometheus and InfluxDB. While cAdvisor itself is not a complete monitoring solution, it is often a component of other monitoring solutions. Before Rancher version 1.2 (late December), Rancher embedded cAdvisor in therancher-agent(for internal use by Rancher), but this is no longer the case. More recent versions of Rancher use Docker stats to gather information exposed through the Rancher UI because they can do so with less overhead. Administrators can easily deploy cAdvisor on Rancher, and it is part of several comprehensive monitoring stacks, but cAdvisor is no longer part of Rancher itself. PROMETHEUS http://prometheus.io Prometheus is a popular, open-source monitoring and alerting toolkit originally built at SoundCloud. It is now a CNCF project, the company’s second hosted project after Kubernetes. As a toolkit, it is substantially different from monitoring solutions described thus far. A first major difference is that rather being offered as a cloud service, Prometheus is modular and self-hosted, meaning that users deploy Prometheus on their clusters whether on-premises or cloud-resident. Rather than pushing data to a cloud service, Prometheus installs on each Docker host and pulls or “scrapes” data from an extensive variety of exporters available to Prometheus via HTTP. Some exporters are officially maintained as a part of the Prometheus GitHub project, while others are external contributions. Some projects expose Prometheus metrics natively so that exporters are not needed. Prometheus is highly extensible. Users need to mind the number of exporters and configure polling intervals appropriately depending on the amount of data they are collecting. The Prometheus server retrieves time-series data from various sources and stores data in its internal datastore. Prometheus provides features like service discovery, a separate push gateway for specific types of metrics and has an embedded query language (PromQL) that excels at querying multidimensional data. It also has an embedded web UI and API. The web UI in Prometheus provides good functionality but relies on users knowing PromQL, so some sites prefer to use Grafana as an interface for charting and viewing cluster-related metrics. Prometheus has a discrete Alert Manager with a distinct UI that can work with data stored in Prometheus. Like other alert managers, it works with a variety of external alerting services including email, Hipchat, Pagerduty, #Slack, OpsGenie, VictorOps, and others. Because Prometheus is comprised of many components, and exporters need to be selected and installed depending on the services monitored, it is more difficult to install; but as a free offering, the price is right. While not quite as refined as tools like Datadog or Sysdig, Prometheus offers similar functionality, extensive third-party software integrations, and best-in-class cloud monitoring solutions. Prometheus is aware of Kubernetes and other container management frameworks. An entry in the Rancher Catalog developed by Infinityworks makes getting started with Prometheus easier when Cattle is used as the Rancher orchestrator but, because of the wide variety of configuration options, administrators need to spend some time to get it properly installed and configured. Infinityworks have contributed useful add-ons including theprometheus-rancher-exporter that exposes the health of Rancher stacks and hosts obtained from the Rancher API to a Prometheus compatible endpoint. For administrators who don’t mind going to a little more effort, Prometheus is one of the most capable monitoring solutions and should be on your shortlist for consideration. HEAPSTER https://github.com/kubernetes/heapster Heapster is another solution that often comes up related to monitoring-container environments. Heapster is a project under the Kubernetes umbrella that helps enable container-cluster monitoring and performance analysis. Heapster specifically supports Kubernetes and OpenShift and is most relevant for Rancher users running Kuberenetes as their orchestrator. It is not typically be used with Cattle or Swarm. People often describe Heapster as a monitoring solution, but it is more precisely a “cluster-wide aggregator of monitoring and event data.” Heapster is never deployed alone; rather, it is a part of a stack of open-source components. The Heapster monitoring stack is typically comprised of: A data gathering tier – e.g., cAdvisor accessed with thekubeleton each cluster host Pluggable storage backends – e.g., ElasticSearch, InfluxDB, Kafka, Graphite, or roughly a dozen others A data visualization component – Grafana or Google Cloud Monitoring A popular stack is comprised of Heapster, InfluxDB, and Grafana, and this combination is installed by default on Rancher when users choose to deploy Kubernetes. Note that these components are considered add-ons to Kubernetes, so they may not be automatically deployed with all Kubernetes distributions. One of the reasons that InfluxDB is popular is that it is one of the few data backends that supports both Kubernetes events and metrics, allowing for more comprehensive monitoring of Kubernetes. Note that Heapster does not natively support alerting or services related to Application Performance Management (APM) found in commercial cloud-based solutions or Prometheus. Users that need monitoring services can supplement their Heapster installation using Hawkular, but this is not automatically configured as part of the Rancher deployment and will require extra user effort. "},"dockerrong-qi-de-jian-kong/ji-yu-cadvisor-de-rong-qi-jian-kong.html":{"url":"dockerrong-qi-de-jian-kong/ji-yu-cadvisor-de-rong-qi-jian-kong.html","title":"基于CAdvisor的容器监控","keywords":"","body":"CAdvisor部署 CAdviosr是Google用来监测单节点的资源信息的监控工具, 提供了一目了然的单节点多容器的图表型资源监控功能。 CAdvisor是一个容器资源监控工具，包括容器的内存，CPU，网络IO，磁盘IO等监控，同时提供了一个WEB页面用于查看容器的实时运行状态。CAdvisor默认存储2分钟的数据，而且只是针对单物理机。 由于CAdvisor已经容器化，部署和运行很简单，执行如下命令即可: $~ docker run \\ --volume=/:/rootfs:ro \\ --volume=/var/run:/var/run:rw \\ --volume=/sys:/sys:ro \\ --volume=/var/lib/docker/:/var/lib/docker:ro \\ --publish=8099:8080 \\ --detach=true \\ --name=cadvisor \\ google/cadvisor:latest 运行之后，就可以在浏览器打开 http://ip:8099 查看宿主机的容器监控数据了。 以上就是 cAdvisor 的主要功能，总结起来主要两点： 展示 Host 和容器两个层次的监控数据。 展示历史变化数据。 cAdvisor 提供的操作界面略显简陋，而且需要在不同页面之间跳转，并且只能监控一个 host。但 cAdvisor 的一个亮点是它可以将监控到的数据导出给第三方工具，由这些工具进一步加工处理。我们可以把 cAdvisor 定位为一个监控数据收集器，收集和导出数据是它的强项，而非展示数据。 CAdvisor原理简介 CAdvisor运行时挂载了宿主机根目录，docker根目录等多个目录，由此可以从中读取容器的运行时信息。docker基础技术有Linux namespace，Control Group(CGroup)，AUFS等，其中CGroup用于系统资源限制和优先级控制的。 宿主机的/sys/fs/cgroup/目录下面存储的就是CGroup的内容了，CGroup包括多个子系统，如对块设备的blkio，cpu，内存，网络IO等限制。Docker在CGroup里面的各个子系统中创建了docker目录，而CAdvisor运行时挂载了宿主机根目录和 /sys目录，从而CAdvisor可以读取到容器的资源使用记录。比如下面可以看到容器206cc当前时刻的CPU的使用统计。 root@ubuntu:/sys/fs/cgroup/cpu/docker# cat 206cc04057e7b855a97b371e0a756a4dc3ea482045de0c95c47dddf8c7925caf/cpuacct.stat user 1696 system 581 而容器网络流量CAdvisor是从/proc/PID/net/dev中读取的，通过docker inspect 206cc可以获取上面容器206cc进程在宿主机的PID为12228，可以看到容器所有网卡的接收和发送流量以及错误数等。CAdvisor定期读取对应目录下面的数据并定期发送到指定的存储引擎存储，而本地会默认存储最近2分钟的数据并提供UI界面查看。 root@ubuntu:~# cat /proc/12228/net/dev Inter-| Receive | Transmit face |bytes packets errs drop fifo frame compressed multicast|bytes packets errs drop fifo colls carrier compressed lo: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 eth0: 10154 88 0 0 0 0 0 0 0 0 0 0 0 0 0 0 "},"dockerrong-qi-de-jian-kong/ji-yu-cadvisor-+-influxdb-+-grafna-de-rong-qi-jian-kong.html":{"url":"dockerrong-qi-de-jian-kong/ji-yu-cadvisor-+-influxdb-+-grafna-de-rong-qi-jian-kong.html","title":"基于Cadvisor+InfluxDB+Grafna的容器监控","keywords":"","body":"CAdvisor是一个容器资源监控工具，包括容器的内存，CPU，网络IO，磁盘IO等监控，同时提供了一个WEB页面用于查看容器的实时运行状态。CAdvisor默认存储2分钟的数据，而且只是针对单物理机。不过，CAdvisor提供了很多数据集成接口，支持InfluxDB，Redis，Kafka，Elasticsearch等集成，可以加上对应配置将监控数据发往这些数据库存储起来。 跨多台主机上容器的监控 cAdivsor虽然能采集到监控数据，也有很好的界面展示，但是并不能显示跨主机的监控数据，当主机多的情况，需要有一种集中式的管理方法将数据进行汇总展示，最经典的方案就是 cAdvisor+ Influxdb+grafana，可以在每台主机上运行一个cAdvisor容器负责数据采集，再将采集后的数据都存到时序型数据库influxdb中，再通过图形展示工具grafana定制展示面板。结构如下： 这三个工具的安装也非常简单，可以直接启动三个容器快速安装。 InfluxDB重要概念 InfluxDB是一个开源的分布式时序数据库，使用GO语言开发。特别适合用于时序类型数据存储，CAdvisor搜集的容器监控数据用InfluxDB存储就很合适，而且CAdvisor本身就提供了InfluxDB的支持，集成起来非常方便。influxdb有一些重要概念：database，timestamp，field key， field value， field set，tag key，tag value，tag set，measurement， retention policy ，series，point，下面简要说明一下： database：数据库，如之前创建的数据库 cadvisor。InfluxDB不是CRUD数据库，更像是一个CR-ud数据库，它优先考虑的是增加和读取数据而不是更新删除数据的性能。 timestamp：时间戳，因为InfluxDB是时序数据库，它的数据里面都有一列名为time的列，存储记录生成时间。如 rx_bytes 中的 time 列，存储的就是时间戳。 fields: 包括field key，field value和field set几个概念。field key是字段名，在rx_bytes表中，字段名为 value。field value是字段值，如 17858781633，1359398等。而field set是字段集合，由field key和field value构成，如rx_bytes中的字段集合如下： value = 17858781633 value = 1359398 在InfluxDB表中，字段必须存在，而且字段是没有索引的。所以，字段相当于传统数据库中没有索引的列。 tags：包括tag key， tag value， tag set几个概念。tag key是标签名，在rx_bytes表中，container_name, game, machine, namespace，type都是标签。tag value就是标签的值了。tag set就是标签集合，由tag key和tag value构成。InfluxDB中标签是可选的，不过标签是有索引的。如果查询中经常用的字段，建议设置为标签而不是字段。标签相当于传统数据库中有索引的列。 retention policy: 数据保留策略，cadvisor的保留策略为cadvisor_retention，存储30天，副本为1。一个数据库可以有多个保留策略。 measurement：类似传统数据看的表，是字段，标签以及time列的集合。 series：共享同一个retention policy，measurement以及tag set的数据集合。 point：同一个series中具有相同时间的字段集合，相当于SQL中的数据行。 InfluxDB的特色功能 InfluxDB作为时序数据库，相比传统数据库它有很多特色功能，比如独有的一些特色函数和连续查询功能。关于InfluxDB的更多详细内容可以参见官方文档。 特色函数：有一些聚合类函数如FILL()用于填充数据, INTEGRAL()计算字段所覆盖的曲面面积，SPREAD()计算表中最大与最小值的差值， STDDEV()计算字段标准差，MEAN()计算平均值, MEDIAN()计算中位数，SAMPLE()函数用于随机取样以及DERIVATIVE()计算数据变化比等。 连续查询：InfluxDB独有的连续查询功能可以定期的缩小取样，就原数据库的数据缩小取样后存储到指定的新的数据库或者新的数据表中，在历史数据统计整理时特别有用。 基于docker-compose.yml文件安装InfluxDB、influxDB、Grafana 基于下面的docker-compose.yml文件安装这三个组件： version: '2' services: influxdbData: image: busybox volumes: - /disk4/influxdb:/data influxdb: image: influxdb restart: always environment: - PRE_CREATE_DB=cadvisor ports: - \"8083:8083\" - \"8086:8086\" expose: - \"8090\" - \"8099\" volumes_from: - \"influxdbData\" cadvisor: image: google/cadvisor links: - influxdb:influxsrv command: -storage_driver=influxdb -storage_driver_db=cadvisor -storage_driver_host=influxsrv:8086 restart: always ports: - \"8080:8080\" volumes: - /:/rootfs:ro - /var/run:/var/run:rw - /sys:/sys:ro - /var/lib/docker/:/var/lib/docker:ro grafana: image: grafana/grafana restart: always links: - influxdb:influxsrv ports: - \"3001:3000\" environment: - HTTP_USER=admin - HTTP_PASS=admin - INFLUXDB_HOST=influxsrv - INFLUXDB_PORT=8086 - INFLUXDB_NAME=cadvisor - INFLUXDB_USER=root - INFLUXDB_PASS=root 为了存储CAdvisor的数据，需要预先创建好数据库并配置用户名密码以及相关权限。InfluxDB提供了一套influx的CLI，跟mysql client很相似。另外，InfluxDB的数据库操作语言InfluxQL跟SQL语法也基本一致。进入InfluxDB容器，运行下面命令创建数据库和用户密码并授权。 root@ubuntu:~/workspace/docker-monitor/cadvisor-influxdb-grafana# docker exec -it c4d /bin/bash root@c4ddd2985191:/# influx Connected to http://localhost:8086 version 1.3.6 InfluxDB shell version: 1.3.6 > create database cadvisor ## 创建数据库cadvisor > show databases name: databases name ---- _internal cadvisor > use cadvisor > CREATE USER 'root' WITH PASSWORD 'root' WITH ALL PRIVILEGES 配置成功后，可以看到CAdvisor会通过InfluxDB的HTTP API自动创建好数据表，并将数据发送到InfluxDB存储起来。 root@c4ddd2985191:/# influx Connected to http://localhost:8086 version 1.3.6 InfluxDB shell version: 1.3.6 > use cadvisor Using database cadvisor > show measurements # 显示数据表与SQL略有不同，用的是关键字measurements name: measurements name ---- cpu_usage_per_cpu cpu_usage_system cpu_usage_total cpu_usage_user fs_limit fs_usage load_average memory_usage memory_working_set rx_bytes rx_errors tx_bytes tx_errors > select * from rx_bytes order by time desc limit 2 name: rx_bytes time Description Vendor Version com.docker.compose.config-hash com.docker.compose.container-number com.docker.compose.oneoff com.docker.compose.project com.docker.compose.service com.docker.compose.version com.docker.stack.namespace com.docker.swarm.node.id com.docker.swarm.service.id com.docker.swarm.service.name com.docker.swarm.task.id com.docker.swarm.task.name container_name machine maintainer value version ---- ----------- ------ ------- ------------------------------ ----------------------------------- ------------------------- -------------------------- -------------------------- -------------------------- -------------------------- ------------------------ --------------------------- ----------------------------- ------------------------ -------------------------- -------------- ------- ---------- ----- ------- 1509002311661632942 /system.slice/lightdm.service 2c8aeeb3c07c 0 1509002311538573198 71cb7f4e7596e83e7e8cf7e44c2d14a44edc08e0f23161baa9dfaa582c4dfa81 1 False logcollectiondocker web 1.16.1 logcollectiondocker_web_1 2c8aeeb3c07c 7803 接下来需要做的就是在Grafana中配置InfluxDB数据源，登录http://localhost:3001 ： InfluxDB数据源配置完成后，创建dashboard，选择上面创建的数据源，配置相应的指标监控panel。 "},"dockerrong-qi-de-jian-kong/ji-yu-prometheus-de-rong-qi-jian-kong.html":{"url":"dockerrong-qi-de-jian-kong/ji-yu-prometheus-de-rong-qi-jian-kong.html","title":"基于Prometheus的容器监控","keywords":"","body":"Prometheus是一款开源的系统监控和告警工具，最初由SoundCloud推出。自2012成立以来，许多公司和组织都采用了prometheus，项目有一个非常活跃的开发者和用户社区。Prometheus现在是一个独立的开源项目，它的维护独立于任何公司，2016年，Prometheus在Kubernetes之后第二个加入到 Cloud Native Computing Foundation 项目。 Features Prometheus的主要特性有： 多维度数据模型（由键/值对确定的时间序列数据模型） 具有一个灵活的查询语言来利用这些维度 不依赖分布式存储；可单个服务器节点工作。 时间序列的采集是通过HTTP pull的形式，解决很多push架构的问题。 通过中介网关支持push形式时间序列数据的收集( pushing time series is supported via an intermediary gateway ) 监控目标的发现是通过服务发现或静态配置( targets are discovered via service discovery or static configuration ) 多种数据展示面板支持，例如grafana ( multiple modes of graphing and dashboarding support ) Architecture This diagram illustrates the architecture of Prometheus and some of its ecosystem components: Prometheus scrapes metrics from instrumented jobs, either directly or via an intermediary push gateway for short-lived jobs. It stores all scraped samples locally and runs rules over this data to either aggregate and record new time series from existing data or generate alerts. Grafana or other API consumers can be used to visualize the collected data. Prometheus Server Prometheus Server 负责从 Exporter 拉取和存储监控数据，并提供一套灵活的查询语言（PromQL）供用户使用。 Exporter Exporter 负责收集目标对象（host, container…）的性能数据，并通过 HTTP 接口供 Prometheus Server 获取。 Push gateway push gateway 用来支持 short-lived jobs 可视化组件 监控数据的可视化展现对于监控方案至关重要。以前 Prometheus 自己开发了一套工具，不过后来废弃了，因为开源社区出现了更为优秀的产品 Grafana。Grafana 能够与 Prometheus 无缝集成，提供完美的数据展示能力。 Alertmanager 用户可以定义基于监控数据的告警规则，规则会触发告警。一旦 Alermanager 收到告警，会通过预定义的方式发出告警通知。支持的方式包括 Email、PagerDuty、Webhook 等. 也许一些熟悉其他监控方案的同学看了 Prometheus 的架构会不以为然，“这些功能 Zabbix、Graphite、Nagios 这类监控系统也都有，没什么特别的啊！”。Prometheus 最大的亮点和先进性是它的多维数据模型，下面将做介绍。 多维数据模型 本节讨论 Prometheus 的核心，多维数据模型。我们先来看一个例子。 比如要监控容器webapp1的内存使用情况，最传统和典型的方法是定义一个指标container_memory_usage_bytes_webapp1来记录webapp1的内存使用数据。假如每1分钟取一次样，那么在数据库里就会有类似下面的记录。 如果现在需求发生了点变化，我们需要知道所有 webapp 容器的内存使用情况。如果还是采用前面的方法，就不得不增加新的指标container_memory_usage_bytes_webapp2、container_memory_usage_bytes_webapp3… 像 Graphite 这类监控方案采用了更为优雅的层次化数据模型。为了满足上面的需求，Graphite 会定义指标container.memory_usage_bytes.webapp1、container.memory_usage_bytes.webapp2、container.memory_usage_bytes.webapp3… 然后就可以用container.memory_usage_bytes.webapp*获取所有的 webapp 的内存使用数据。 此外，Graphite 还支持sum()等函数对指标进行计算和处理，比如sum(container.memory_usage_bytes.webapp*)可以得到所有 webapp 容器占用的总内存量。 目前为止问题处理得都很好。但客户总是会提出更多的需求：现在不仅要按容器名字统计内存使用量，还要按镜像来统计；或者想对比一下某一组容器在生产环境和测试环境中对内存使用的不同情况。 当然你可以说：只要定义更多的指标就能满足这些需求。比 如container.memory_usage_bytes.image1.webapp1、container.memory_usage_bytes.webapp1.prod等。 但问题在于我们没办法提前预知客户要用这些数据回答怎样的问题，所以我们没办法提前定义好所有的指标。 下面来看看 Prometheus 的解决方案。 Prometheus 只需要定义一个全局的指标container_memory_usage_bytes，然后通过添加不同的维度数据来满足不同的业务需求。 比如对于前面 webapp1 的三条取样数据，转换成 Prometheus 多维数据将变成： 后面三列container_name、image、env就是数据的三个维度。想象一下，如果不同env（prod、test、dev），不同image（mycom/webapp:1.2、mycom/webapp:1.3）的容器，它们的内存使用数据中标注了这三个维度信息，那么将能满足很多业务需求，比如： 计算 webapp2 的平均内存使用情况：avg(container_memory_usage_bytes{container_name=“webapp2”}) 计算运行 mycom/webapp:1.3 镜像的所有容器内存使用总量：sum(container_memory_usage_bytes{image=“mycom/webapp:1.3”}) 统计不同运行环境中 webapp 容器内存使用总量：sum(container_memory_usage_bytes{container_name=~“webapp”}) by (env) 这里只列了几个例子，不过已经能够说明 Prometheus 数据模型的优势了： 通过维度对数据进行说明，附加更多的业务信息，进而满足不同业务的需求。同时维度是可以动态添加的，比如再给数据加上一个user维度，就可以按用户来统计容器内存使用量了。 Prometheus 丰富的查询语言能够灵活、充分地挖掘数据的价值。前面示例中的 avg、sum、by 只是查询语言中很小的一部分功能，已经为我们展现了 Prometheus 对多维数据进行分片、聚合的强大能力。 基于Prometheus的容器监控部署 基于CAdvisor的exporter，Prometheus的部署使用docker-compose.yml文件将多个容器统一部署： version: '2' services: prometheus: image: prom/prometheus volumes: - ./prometheus/:/etc/prometheus/ - ./data/prometheus:/prometheus command: - '-config.file=/etc/prometheus/prometheus.yml' - '-storage.local.path=/prometheus' - '-alertmanager.url=http://alertmanager:9093' - '-web.console.libraries=/usr/share/prometheus/console_libraries' - '-web.console.templates=/usr/share/prometheus/consoles' ports: - 9090:9090 links: - cadvisor:cadvisor - alertmanager:alertmanager depends_on: - cadvisor restart: always node-exporter: image: prom/node-exporter volumes: - /proc:/host/proc - /sys:/host/sys - /:/rootfs command: - '--path.procfs=/host/proc' - '--path.sysfs=/host/sys' - '--collector.filesystem.ignored-mount-points=\"^/(sys|proc|dev|host|etc)($$|/)\"' ports: - 9100:9100 restart: always alertmanager: image: prom/alertmanager ports: - 9093:9093 volumes: - ./alertmanager/:/etc/alertmanager/ restart: always command: - '-config.file=/etc/alertmanager/config.yml' - '-storage.path=/alertmanager' cadvisor: image: google/cadvisor volumes: - /:/rootfs:ro - /var/run:/var/run:rw - /sys:/sys:ro - /var/lib/docker/:/var/lib/docker:ro ports: - 8011:8080 restart: always grafana: image: grafana/grafana depends_on: - prometheus ports: - 3009:3000 volumes: - ./data/grafana:/var/lib/grafana env_file: - config.monitoring 执行docker-compose up -d ，该方案采用的也是使用grafana作为可视化组件，可以在浏览器中通过访问http://localhost:3009 进入Grafana主页, 然后配置Prometheus的数据源，并导入一个docker-dashboard.json的dashboard模板，可以看到如下图的监控效果图： 参考资料 https://grafana.com/dashboards/179 https://grafana.com/dashboards/893 https://grafana.com/dashboards?dataSource=prometheus&search=docker https://github.com/vegasbrianc/prometheus/tree/version-2 "}}